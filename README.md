# Clustering
Using DBSCAN and Kmeans to cluster data.
In this project I used Scikit Learn’s Kmeans and DBSCAN in order to cluster two different datasets. I attempted to minimize the inertia (Sum of Squared Errors) in my Kmeans clustering of both datasets using different values of k in order to get a more reliable clustering. I also experimented with different values of epsilon and minimum samples for my DBSCAN clustering and used silhouette scores and correlation to test the validity of the clusters being created. Kmeans clustering requires data input and number of clusters. Scikit Learns method k_means returns the centroids, labels, and SSE for each data input and number of clusters. I determined SSE on dataset 1 and dataset to on a k interval from 2 to 5 and found the smallest amount of error in both datasets occurred with 5 clusters. The smallest SSE for dataset 1 was 29208.5367 with 5 clusters, and the smallest SSE on dataset 2 was 190.4062 with 5 clusters. These were chosen as the best labeled clusters out of the range for Kmeans. I believe 5 clusters is a good choice for these datasets because it allows for more diversity of the data and reduces noisy clusters. DBSCAN clusters data based on a distance threshold called epsilon and a variable that tells the clustering algorithm the minimum number of samples allowed in order to consider a group of samples a cluster. I found the best results through trial and error. For dataset 1 I chose epsilon of 9 and used the default setting of 5 min samples to get my best clustering. For dataset 2 I chose epsilon of 0.28 and min samples of 2.  I believe the difference here had to do with the datasets themselves as dataset 1 has much larger numbers and the data may be more spread apart but better correlated while dataset two has small numbers that may be closer together according to a distance metric but not well correlated enough to put many samples in a cluster. The DBSCAN on dataset 1 has 3 clusters while the DBSCAN on dataset 2 has around 11, however, I believe this number of clusters was necessary to reduce noise. I validated all of my clusters with correlation and silhouette scores. The silhouette scores were computed with SciKit Learn’s silhouette_score metric. The Kmeans average silhouette score for dataset 1 was 0.3777 and for dataset 2 it was 0.1586. The DBSCAN average silhouette score for dataset 1 was 0.7979 and for dataset 2 it was -0.2983. These results are not surprising to me as the silhouette score measures how similar an object is to its own cluster compared to other clusters in a range of -1 to 1. This shows that dataset 1 had more similar samples than dataset 2 and that DBSCAN produced the most reliable clustering.  In order to calculate the correlation for each cluster I used the formula: Covariance(X,Y)/(Standard Deviation(X)*Standard Deviation(Y)) Where X is the upper triangle of my proximity matrix for each cluster, Y is the upper triangle of my incidence matrix for each cluster. I used scipy’s distance_matrix function to compute my proximity matrix on dataset 1 and dataset 2. In order to get my incidence matrices I used a for loop to compare every sample’s cluster to every other cluster and generated matrices of 750x750 of 1’s for every associated pair of points in the same cluster and 0’s for pairs that belonged to different clusters. I got the upper triangle of these matrices using numpy’s function triu_indices for 750 points and axis increase of 1 to remove the redundant comparisons. This resulted in a list of 280875 values for proximity and the same number for incidence. I then collected the mean and standard deviation of my X and the mean and standard deviation of my Y with numpy’s mean and std functions. This allowed me to compute my covariance using the formula: Σ((X-x_mean)(Y-y_mean))/N Where X and Y are still the upper triangles of my proximity matrices and incidence matrices respectively and N is the number of values (in my case 280875). Once I had my covariance and standard deviations I was able to compute the correlation for each clustering. Kmeans produced a correlation of  -0.6197 for dataset 1 and -0.4130 for dataset 2. DBSCAN produced correlation of  -0.8085 for dataset 1 and -0.1904 for dataset 2. From these results we can affirm that dataset 1 was better correlated than dataset 2 and that DBSCAN had the most reliable clustering of the well correlated data.      
